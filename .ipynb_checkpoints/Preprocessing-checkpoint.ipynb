{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 159571\n",
      "0    Explanation\\nWhy the edits made under my usern...\n",
      "1    D'aww! He matches this background colour I'm s...\n",
      "Name: comment_text, dtype: object 0    0\n",
      "1    0\n",
      "Name: toxic, dtype: int64 0    0\n",
      "1    0\n",
      "Name: severe_toxic, dtype: int64 0    0\n",
      "1    0\n",
      "Name: obscene, dtype: int64 0    0\n",
      "1    0\n",
      "Name: threat, dtype: int64 0    0\n",
      "1    0\n",
      "Name: insult, dtype: int64 0    0\n",
      "1    0\n",
      "Name: identity_hate, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of entries:\",len(df))\n",
    "\n",
    "# assigning class_label corresponding to index of class\n",
    "\n",
    "class_label=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "comments=df.comment_text\n",
    "toxic= df.toxic\n",
    "severe_toxic= df.severe_toxic\n",
    "obscene= df.obscene\n",
    "threat= df.threat\n",
    "insult= df.insult\n",
    "identity_hate= df.identity_hate\n",
    "\n",
    "print(comments[:2], toxic[:2], severe_toxic[:2],obscene[:2],threat[:2],insult[:2],identity_hate[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning- removing punctuations, numbers, special characters, converting to lower case\n",
    "import string\n",
    "clean_text=[]\n",
    "for i in comments:\n",
    "    texts=i.split(' ')\n",
    "    arr=[]\n",
    "    for j in texts:\n",
    "        j=j.translate(str.maketrans('', '', string.punctuation))\n",
    "        j=''.join([i for i in j if not i.isdigit()])\n",
    "        j=j.lower()\n",
    "        if \"\\n\" in j:\n",
    "            brr=j.split('\\n')\n",
    "            arr+=brr\n",
    "        else:\n",
    "            arr.append(j)\n",
    "    clean_text.append(arr)\n",
    "\n",
    "clean_text_clean=[]\n",
    "\n",
    "hash_list={}\n",
    "hash_list_reverse={}\n",
    "hashed_clean_text=[]\n",
    "counter=0\n",
    "for i in clean_text:\n",
    "    arr=[]\n",
    "    hashed_clean_text_arr=[]\n",
    "    for j in i:\n",
    "        if j!=\"\":\n",
    "            arr.append(j)\n",
    "            if j not in hash_list:\n",
    "                hash_list[j]=counter\n",
    "                hash_list_reverse[counter]=j\n",
    "                counter+=1\n",
    "            hashed_clean_text_arr.append(hash_list[j])\n",
    "    clean_text_clean.append(arr)\n",
    "    hashed_clean_text.append(hashed_clean_text_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['explanation', 'why', 'the', 'edits', 'made', 'under', 'my', 'username', 'hardcore', 'metallica', 'fan', 'were', 'reverted', 'they', 'werent', 'vandalisms', 'just', 'closure', 'on', 'some', 'gas', 'after', 'i', 'voted', 'at', 'new', 'york', 'dolls', 'fac', 'and', 'please', 'dont', 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', 'im', 'retired', 'now'], ['daww', 'he', 'matches', 'this', 'background', 'colour', 'im', 'seemingly', 'stuck', 'with', 'thanks', 'talk', 'january', 'utc'], ['hey', 'man', 'im', 'really', 'not', 'trying', 'to', 'edit', 'war', 'its', 'just', 'that', 'this', 'guy', 'is', 'constantly', 'removing', 'relevant', 'information', 'and', 'talking', 'to', 'me', 'through', 'edits', 'instead', 'of', 'my', 'talk', 'page', 'he', 'seems', 'to', 'care', 'more', 'about', 'the', 'formatting', 'than', 'the', 'actual', 'info'], ['more', 'i', 'cant', 'make', 'any', 'real', 'suggestions', 'on', 'improvement', 'i', 'wondered', 'if', 'the', 'section', 'statistics', 'should', 'be', 'later', 'on', 'or', 'a', 'subsection', 'of', 'types', 'of', 'accidents', 'i', 'think', 'the', 'references', 'may', 'need', 'tidying', 'so', 'that', 'they', 'are', 'all', 'in', 'the', 'exact', 'same', 'format', 'ie', 'date', 'format', 'etc', 'i', 'can', 'do', 'that', 'later', 'on', 'if', 'noone', 'else', 'does', 'first', 'if', 'you', 'have', 'any', 'preferences', 'for', 'formatting', 'style', 'on', 'references', 'or', 'want', 'to', 'do', 'it', 'yourself', 'please', 'let', 'me', 'know', 'there', 'appears', 'to', 'be', 'a', 'backlog', 'on', 'articles', 'for', 'review', 'so', 'i', 'guess', 'there', 'may', 'be', 'a', 'delay', 'until', 'a', 'reviewer', 'turns', 'up', 'its', 'listed', 'in', 'the', 'relevant', 'form', 'eg', 'wikipediagoodarticlenominationstransport'], ['you', 'sir', 'are', 'my', 'hero', 'any', 'chance', 'you', 'remember', 'what', 'page', 'thats', 'on'], ['congratulations', 'from', 'me', 'as', 'well', 'use', 'the', 'tools', 'well', '\\xa0·', 'talk'], ['cocksucker', 'before', 'you', 'piss', 'around', 'on', 'my', 'work'], ['your', 'vandalism', 'to', 'the', 'matt', 'shirvington', 'article', 'has', 'been', 'reverted', 'please', 'dont', 'do', 'it', 'again', 'or', 'you', 'will', 'be', 'banned'], ['sorry', 'if', 'the', 'word', 'nonsense', 'was', 'offensive', 'to', 'you', 'anyway', 'im', 'not', 'intending', 'to', 'write', 'anything', 'in', 'the', 'articlewow', 'they', 'would', 'jump', 'on', 'me', 'for', 'vandalism', 'im', 'merely', 'requesting', 'that', 'it', 'be', 'more', 'encyclopedic', 'so', 'one', 'can', 'use', 'it', 'for', 'school', 'as', 'a', 'reference', 'i', 'have', 'been', 'to', 'the', 'selective', 'breeding', 'page', 'but', 'its', 'almost', 'a', 'stub', 'it', 'points', 'to', 'animal', 'breeding', 'which', 'is', 'a', 'short', 'messy', 'article', 'that', 'gives', 'you', 'no', 'info', 'there', 'must', 'be', 'someone', 'around', 'with', 'expertise', 'in', 'eugenics'], ['alignment', 'on', 'this', 'subject', 'and', 'which', 'are', 'contrary', 'to', 'those', 'of', 'dulithgow']]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(clean_text_clean[:10])\n",
    "\n",
    "# store preprocessed text as json\n",
    "\n",
    "json.dump({\n",
    "    \"clean_text\":clean_text_clean\n",
    "}, open(\"dataset/store_preprocessed.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['explanation', 'why', 'the', 'edits', 'made', 'under', 'my', 'username', 'hardcore', 'metallica', 'fan', 'were', 'reverted', 'they', 'werent', 'vandalisms', 'just', 'closure', 'on', 'some', 'gas', 'after', 'i', 'voted', 'at', 'new', 'york', 'dolls', 'fac', 'and', 'please', 'dont', 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', 'im', 'retired', 'now'], ['daww', 'he', 'matches', 'this', 'background', 'colour', 'im', 'seemingly', 'stuck', 'with', 'thanks', 'talk', 'january', 'utc'], ['hey', 'man', 'im', 'really', 'not', 'trying', 'to', 'edit', 'war', 'its', 'just', 'that', 'this', 'guy', 'is', 'constantly', 'removing', 'relevant', 'information', 'and', 'talking', 'to', 'me', 'through', 'edits', 'instead', 'of', 'my', 'talk', 'page', 'he', 'seems', 'to', 'care', 'more', 'about', 'the', 'formatting', 'than', 'the', 'actual', 'info'], ['more', 'i', 'cant', 'make', 'any', 'real', 'suggestions', 'on', 'improvement', 'i', 'wondered', 'if', 'the', 'section', 'statistics', 'should', 'be', 'later', 'on', 'or', 'a', 'subsection', 'of', 'types', 'of', 'accidents', 'i', 'think', 'the', 'references', 'may', 'need', 'tidying', 'so', 'that', 'they', 'are', 'all', 'in', 'the', 'exact', 'same', 'format', 'ie', 'date', 'format', 'etc', 'i', 'can', 'do', 'that', 'later', 'on', 'if', 'noone', 'else', 'does', 'first', 'if', 'you', 'have', 'any', 'preferences', 'for', 'formatting', 'style', 'on', 'references', 'or', 'want', 'to', 'do', 'it', 'yourself', 'please', 'let', 'me', 'know', 'there', 'appears', 'to', 'be', 'a', 'backlog', 'on', 'articles', 'for', 'review', 'so', 'i', 'guess', 'there', 'may', 'be', 'a', 'delay', 'until', 'a', 'reviewer', 'turns', 'up', 'its', 'listed', 'in', 'the', 'relevant', 'form', 'eg', 'wikipediagoodarticlenominationstransport'], ['you', 'sir', 'are', 'my', 'hero', 'any', 'chance', 'you', 'remember', 'what', 'page', 'thats', 'on'], ['congratulations', 'from', 'me', 'as', 'well', 'use', 'the', 'tools', 'well', '\\xa0·', 'talk'], ['cocksucker', 'before', 'you', 'piss', 'around', 'on', 'my', 'work'], ['your', 'vandalism', 'to', 'the', 'matt', 'shirvington', 'article', 'has', 'been', 'reverted', 'please', 'dont', 'do', 'it', 'again', 'or', 'you', 'will', 'be', 'banned'], ['sorry', 'if', 'the', 'word', 'nonsense', 'was', 'offensive', 'to', 'you', 'anyway', 'im', 'not', 'intending', 'to', 'write', 'anything', 'in', 'the', 'articlewow', 'they', 'would', 'jump', 'on', 'me', 'for', 'vandalism', 'im', 'merely', 'requesting', 'that', 'it', 'be', 'more', 'encyclopedic', 'so', 'one', 'can', 'use', 'it', 'for', 'school', 'as', 'a', 'reference', 'i', 'have', 'been', 'to', 'the', 'selective', 'breeding', 'page', 'but', 'its', 'almost', 'a', 'stub', 'it', 'points', 'to', 'animal', 'breeding', 'which', 'is', 'a', 'short', 'messy', 'article', 'that', 'gives', 'you', 'no', 'info', 'there', 'must', 'be', 'someone', 'around', 'with', 'expertise', 'in', 'eugenics'], ['alignment', 'on', 'this', 'subject', 'and', 'which', 'are', 'contrary', 'to', 'those', 'of', 'dulithgow']]\n"
     ]
    }
   ],
   "source": [
    "# importing dataset (if needed)\n",
    "import json\n",
    "f=open(\"dataset/store_preprocessed.json\", \"r\")\n",
    "\n",
    "clean_text= json.load(f)\n",
    "print(clean_text[\"clean_text\"][:10])\n",
    "clean_text=clean_text[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing results for each indices in class-index form\n",
    "\n",
    "result=[]\n",
    "for i in range(len(df)):\n",
    "    arr=[]\n",
    "    if toxic[i]==1:\n",
    "        arr.append(0)\n",
    "    if severe_toxic[i]==1:\n",
    "        arr.append(1)\n",
    "    if obscene[i]==1:\n",
    "        arr.append(2)\n",
    "    if threat[i]==1:\n",
    "        arr.append(3)\n",
    "    if insult[i]==1:\n",
    "        arr.append(4)\n",
    "    if identity_hate[i]==1:\n",
    "        arr.append(5)\n",
    "    if arr:\n",
    "        print(clean_text[i], arr)\n",
    "    result.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# storing the hash results\n",
    "\n",
    "json.dump({\n",
    "    \"hash_list\":hash_list,\n",
    "    \"hash_list_reverse\":hash_list_reverse,\n",
    "    \"hashed_clean_text\":hashed_clean_text\n",
    "}, open(\"dataset/hash.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trying to import\n",
    "import json\n",
    "f=open(\"dataset/hash.json\", \"r\")\n",
    "\n",
    "hashed= json.load(f)\n",
    "print(hashed[\"hashed_clean_text\"][:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
